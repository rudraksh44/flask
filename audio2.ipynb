{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1229041,"sourceType":"datasetVersion","datasetId":703366}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport librosa\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Define the emotions\nemotion = {\"Angry\": 0, \"Disgusted\": 1, \"Fearful\": 2, \"Happy\": 3, \"Neutral\": 4, \"Sad\": 5, \"Suprised\": 6}\n\n# MFCC extraction\ndef extract_mfcc(path, pad_len=100):\n    audio, sample_rate = librosa.load(path, res_type='kaiser_fast')\n    mfcc = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n    if mfcc.shape[1] < pad_len:\n        pad_width = pad_len - mfcc.shape[1]\n        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n    else:\n        mfcc = mfcc[:, :pad_len]\n    return mfcc\n\n# Additional features\ndef extract_add(path):\n    audio, sr = librosa.load(path, res_type='kaiser_fast')\n    pitch, _ = librosa.core.piptrack(y=audio, sr=sr)\n    pitch_mean = np.mean(pitch[pitch > 0]) if np.any(pitch > 0) else 0\n    energy = np.mean(librosa.feature.rms(y=audio))\n    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sr))\n    spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=audio, sr=sr))\n    return np.array([pitch_mean, energy, spectral_centroid, spectral_bandwidth])\n\n# Load the dataset\ndef load_data(data_path, emotions):\n    features, add_feature, labels = [], [], []\n    for emotion in emotions:\n        emotion_folder = os.path.join(data_path, emotion)\n        for file in os.listdir(emotion_folder):\n            if file.endswith('.wav'):\n                file_path = os.path.join(emotion_folder, file)\n                mfcc = extract_mfcc(file_path, pad_len=100)\n                features.append(mfcc)\n                labels.append(emotions[emotion])\n                add_feature1 = extract_add(file_path)\n                add_feature.append(add_feature1)\n    features = np.array(features)\n    labels = np.array(labels)\n    features = features.reshape(features.shape[0], features.shape[1], features.shape[2], 1)  # For CNN\n    return features, labels, np.array(add_feature)\n\n# CNN model\n\n\n# Assuming data loading step\nX, y, X1 = load_data(\"/kaggle/input/audio-emotions/Emotions\", emotion)\n\n# Prepare DataLoaders\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nX_tensor = torch.tensor(X, dtype=torch.float32).to(device)\ny_tensor = torch.tensor(y, dtype=torch.long).to(device)\nX_tensor1 = torch.tensor(X1, dtype=torch.float32).to(device)\n\ndataset = TensorDataset(X_tensor, y_tensor)\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Initialize CNN model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T22:10:03.996021Z","iopub.execute_input":"2024-10-18T22:10:03.996583Z","iopub.status.idle":"2024-10-18T22:43:20.760866Z","shell.execute_reply.started":"2024-10-18T22:10:03.996517Z","shell.execute_reply":"2024-10-18T22:43:20.759383Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 113\u001b[0m\n\u001b[1;32m    111\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    112\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Shape [32, 1, 40, 100]\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    115\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[1], line 64\u001b[0m, in \u001b[0;36mEmotionCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[1;32m     63\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[0;32m---> 64\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)))\n\u001b[1;32m     66\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n","\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."],"ename":"RuntimeError","evalue":"view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"X_tensor.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T22:55:11.816524Z","iopub.execute_input":"2024-10-18T22:55:11.817013Z","iopub.status.idle":"2024-10-18T22:55:11.826597Z","shell.execute_reply.started":"2024-10-18T22:55:11.816967Z","shell.execute_reply":"2024-10-18T22:55:11.825046Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"torch.Size([12798, 40, 100, 1])"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"class EmotionCNN(nn.Module):\n    def __init__(self):\n        super(EmotionCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n        self.fc1 = nn.Linear(64 * 10 * 25, 128)\n        self.fc2 = nn.Linear(128, 7)\n        self.dropout = nn.Dropout(p=0.5)\n\n    def forward(self, x):\n        x = self.pool(torch.relu(self.conv1(x)))\n        x = self.pool(torch.relu(self.conv2(x)))\n        x = x.reshape(x.size(0), -1)\n        x = self.dropout(torch.relu(self.fc1(x)))\n        x = self.fc2(x)\n        return x\n\n# LSTM model\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(LSTMModel, self).__init__()\n        \n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.3, bidirectional=True)\n        self.fc1 = nn.Linear(hidden_size * 2, 128)  # Multiply by 2 for bidirectional\n        self.dropout1 = nn.Dropout(0.5)\n        self.batch_norm1 = nn.BatchNorm1d(128)\n        self.fc2 = nn.Linear(128, num_classes)\n        \n    def forward(self, x):\n        # LSTM forward pass\n        lstm_out, _ = self.lstm(x)  # lstm_out shape: (batch_size, seq_len, hidden_size * 2)\n        \n        # Use the last hidden state for classification\n        last_hidden_state = lstm_out[:, -1, :]  # Shape: (batch_size, hidden_size * 2)\n        \n        x = self.fc1(last_hidden_state)  # Shape: (batch_size, 128)\n        x = self.dropout1(x)  # Apply dropout\n        x = self.batch_norm1(x)  # Apply batch normalization\n        x = torch.relu(x)  # Activation function\n        \n        x = self.fc2(x)  # Output layer\n        return x\n\n# Parameters\ninput_size = 4  # Number of features (from additional features extraction)\nhidden_size = 64\nnum_layers = 2  # Number of LSTM layers\nnum_classes = 7  # Number of emotion classes\n\n# Instantiate the model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T23:24:55.210998Z","iopub.execute_input":"2024-10-18T23:24:55.211844Z","iopub.status.idle":"2024-10-18T23:24:55.224790Z","shell.execute_reply.started":"2024-10-18T23:24:55.211802Z","shell.execute_reply":"2024-10-18T23:24:55.223803Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"model = EmotionCNN().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop for CNN\nfor epoch in range(100):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        inputs = inputs.permute(0, 3, 1, 2)  # Shape [32, 1, 40, 100]\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    print(f'Epoch [{epoch + 1}/30], Loss: {running_loss / len(train_loader):.4f}')\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T23:01:34.557539Z","iopub.execute_input":"2024-10-18T23:01:34.558770Z","iopub.status.idle":"2024-10-18T23:04:01.511284Z","shell.execute_reply.started":"2024-10-18T23:01:34.558699Z","shell.execute_reply":"2024-10-18T23:04:01.510204Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Epoch [1/30], Loss: 1.9578\nEpoch [2/30], Loss: 1.2110\nEpoch [3/30], Loss: 1.1504\nEpoch [4/30], Loss: 1.0953\nEpoch [5/30], Loss: 1.0486\nEpoch [6/30], Loss: 1.0121\nEpoch [7/30], Loss: 0.9675\nEpoch [8/30], Loss: 0.9283\nEpoch [9/30], Loss: 0.8825\nEpoch [10/30], Loss: 0.8525\nEpoch [11/30], Loss: 0.8192\nEpoch [12/30], Loss: 0.7661\nEpoch [13/30], Loss: 0.7370\nEpoch [14/30], Loss: 0.7152\nEpoch [15/30], Loss: 0.6749\nEpoch [16/30], Loss: 0.6415\nEpoch [17/30], Loss: 0.6253\nEpoch [18/30], Loss: 0.5860\nEpoch [19/30], Loss: 0.5593\nEpoch [20/30], Loss: 0.5521\nEpoch [21/30], Loss: 0.5119\nEpoch [22/30], Loss: 0.5123\nEpoch [23/30], Loss: 0.4893\nEpoch [24/30], Loss: 0.4723\nEpoch [25/30], Loss: 0.4648\nEpoch [26/30], Loss: 0.4370\nEpoch [27/30], Loss: 0.4371\nEpoch [28/30], Loss: 0.4245\nEpoch [29/30], Loss: 0.4014\nEpoch [30/30], Loss: 0.3847\nEpoch [31/30], Loss: 0.3654\nEpoch [32/30], Loss: 0.3891\nEpoch [33/30], Loss: 0.3838\nEpoch [34/30], Loss: 0.3708\nEpoch [35/30], Loss: 0.3516\nEpoch [36/30], Loss: 0.3428\nEpoch [37/30], Loss: 0.3352\nEpoch [38/30], Loss: 0.3442\nEpoch [39/30], Loss: 0.3363\nEpoch [40/30], Loss: 0.3409\nEpoch [41/30], Loss: 0.3125\nEpoch [42/30], Loss: 0.3232\nEpoch [43/30], Loss: 0.2997\nEpoch [44/30], Loss: 0.3054\nEpoch [45/30], Loss: 0.2939\nEpoch [46/30], Loss: 0.2960\nEpoch [47/30], Loss: 0.3027\nEpoch [48/30], Loss: 0.2964\nEpoch [49/30], Loss: 0.2959\nEpoch [50/30], Loss: 0.2747\nEpoch [51/30], Loss: 0.2784\nEpoch [52/30], Loss: 0.2654\nEpoch [53/30], Loss: 0.2848\nEpoch [54/30], Loss: 0.2933\nEpoch [55/30], Loss: 0.2644\nEpoch [56/30], Loss: 0.2513\nEpoch [57/30], Loss: 0.2702\nEpoch [58/30], Loss: 0.2571\nEpoch [59/30], Loss: 0.2499\nEpoch [60/30], Loss: 0.2626\nEpoch [61/30], Loss: 0.2519\nEpoch [62/30], Loss: 0.2514\nEpoch [63/30], Loss: 0.2681\nEpoch [64/30], Loss: 0.2292\nEpoch [65/30], Loss: 0.2240\nEpoch [66/30], Loss: 0.2368\nEpoch [67/30], Loss: 0.2373\nEpoch [68/30], Loss: 0.2380\nEpoch [69/30], Loss: 0.2567\nEpoch [70/30], Loss: 0.2355\nEpoch [71/30], Loss: 0.2236\nEpoch [72/30], Loss: 0.2149\nEpoch [73/30], Loss: 0.2344\nEpoch [74/30], Loss: 0.2254\nEpoch [75/30], Loss: 0.2202\nEpoch [76/30], Loss: 0.2196\nEpoch [77/30], Loss: 0.2300\nEpoch [78/30], Loss: 0.2132\nEpoch [79/30], Loss: 0.2181\nEpoch [80/30], Loss: 0.2073\nEpoch [81/30], Loss: 0.2195\nEpoch [82/30], Loss: 0.1977\nEpoch [83/30], Loss: 0.2312\nEpoch [84/30], Loss: 0.2139\nEpoch [85/30], Loss: 0.1961\nEpoch [86/30], Loss: 0.1930\nEpoch [87/30], Loss: 0.2113\nEpoch [88/30], Loss: 0.1984\nEpoch [89/30], Loss: 0.2016\nEpoch [90/30], Loss: 0.2015\nEpoch [91/30], Loss: 0.2067\nEpoch [92/30], Loss: 0.1956\nEpoch [93/30], Loss: 0.1773\nEpoch [94/30], Loss: 0.2042\nEpoch [95/30], Loss: 0.1960\nEpoch [96/30], Loss: 0.2043\nEpoch [97/30], Loss: 0.2057\nEpoch [98/30], Loss: 0.1958\nEpoch [99/30], Loss: 0.2009\nEpoch [100/30], Loss: 0.1836\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Evaluation loop\n\nmodel.eval()  # Set the model to evaluation mode\ncorrect = 0\ntotal = 0\n\n# Use no_grad() to disable gradient calculation\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs = inputs.permute(0, 3, 1, 2)  # Ensure correct shape [batch_size, channels, height, width]\n        \n        # Forward pass through the model\n        outputs = model(inputs)  # Get model predictions\n        \n        # Get the predicted class (the index of the maximum log-probability)\n        _, predicted = torch.max(outputs.data, 1)\n        \n        # Update total and correct counts\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()  # Count correct predictions\n\n# Print the final accuracy\naccuracy = 100 * correct / total if total > 0 else 0\nprint(f'Accuracy: {accuracy:.2f}%')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T23:23:43.546071Z","iopub.execute_input":"2024-10-18T23:23:43.546471Z","iopub.status.idle":"2024-10-18T23:23:43.589334Z","shell.execute_reply.started":"2024-10-18T23:23:43.546434Z","shell.execute_reply":"2024-10-18T23:23:43.588246Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m---> 10\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure correct shape [batch_size, channels, height, width]\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(inputs)  \u001b[38;5;66;03m# Get model predictions\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 4"],"ename":"RuntimeError","evalue":"permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 4","output_type":"error"}],"execution_count":25},{"cell_type":"code","source":"# Save the model state dictionary\ntorch.save(model.state_dict(), 'emotion_cnn_model.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T23:10:13.739467Z","iopub.execute_input":"2024-10-18T23:10:13.740226Z","iopub.status.idle":"2024-10-18T23:10:13.759965Z","shell.execute_reply.started":"2024-10-18T23:10:13.740184Z","shell.execute_reply":"2024-10-18T23:10:13.759006Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Save the entire model\ntorch.save(model, 'emotion_cnn_model_full.pth')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T23:14:08.578430Z","iopub.execute_input":"2024-10-18T23:14:08.579357Z","iopub.status.idle":"2024-10-18T23:14:08.597905Z","shell.execute_reply.started":"2024-10-18T23:14:08.579315Z","shell.execute_reply":"2024-10-18T23:14:08.597047Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"dataset = TensorDataset(X_tensor1, y_tensor)\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Initialize CNN model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T23:25:54.986962Z","iopub.execute_input":"2024-10-18T23:25:54.987869Z","iopub.status.idle":"2024-10-18T23:25:54.995226Z","shell.execute_reply.started":"2024-10-18T23:25:54.987820Z","shell.execute_reply":"2024-10-18T23:25:54.994249Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"model1 = LSTMModel(input_size, hidden_size, num_layers, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop for CNN\nfor epoch in range(150):\n    model1.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        inputs = inputs.unsqueeze(1)\n        outputs = model1(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    print(f'Epoch [{epoch + 1}/30], Loss: {running_loss / len(train_loader):.4f}')\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T23:26:11.500359Z","iopub.execute_input":"2024-10-18T23:26:11.500972Z","iopub.status.idle":"2024-10-18T23:29:50.327377Z","shell.execute_reply.started":"2024-10-18T23:26:11.500930Z","shell.execute_reply":"2024-10-18T23:29:50.326197Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Epoch [1/30], Loss: 2.0168\nEpoch [2/30], Loss: 2.0125\nEpoch [3/30], Loss: 2.0097\nEpoch [4/30], Loss: 2.0091\nEpoch [5/30], Loss: 2.0087\nEpoch [6/30], Loss: 2.0131\nEpoch [7/30], Loss: 2.0135\nEpoch [8/30], Loss: 2.0145\nEpoch [9/30], Loss: 2.0109\nEpoch [10/30], Loss: 2.0047\nEpoch [11/30], Loss: 2.0115\nEpoch [12/30], Loss: 2.0095\nEpoch [13/30], Loss: 2.0110\nEpoch [14/30], Loss: 2.0083\nEpoch [15/30], Loss: 2.0208\nEpoch [16/30], Loss: 2.0145\nEpoch [17/30], Loss: 2.0110\nEpoch [18/30], Loss: 2.0135\nEpoch [19/30], Loss: 2.0152\nEpoch [20/30], Loss: 2.0051\nEpoch [21/30], Loss: 2.0158\nEpoch [22/30], Loss: 2.0113\nEpoch [23/30], Loss: 2.0075\nEpoch [24/30], Loss: 2.0087\nEpoch [25/30], Loss: 2.0130\nEpoch [26/30], Loss: 2.0127\nEpoch [27/30], Loss: 2.0104\nEpoch [28/30], Loss: 2.0082\nEpoch [29/30], Loss: 2.0105\nEpoch [30/30], Loss: 2.0131\nEpoch [31/30], Loss: 2.0058\nEpoch [32/30], Loss: 2.0086\nEpoch [33/30], Loss: 2.0099\nEpoch [34/30], Loss: 2.0123\nEpoch [35/30], Loss: 2.0088\nEpoch [36/30], Loss: 2.0129\nEpoch [37/30], Loss: 2.0071\nEpoch [38/30], Loss: 2.0139\nEpoch [39/30], Loss: 2.0081\nEpoch [40/30], Loss: 2.0070\nEpoch [41/30], Loss: 2.0134\nEpoch [42/30], Loss: 2.0111\nEpoch [43/30], Loss: 2.0109\nEpoch [44/30], Loss: 2.0133\nEpoch [45/30], Loss: 2.0161\nEpoch [46/30], Loss: 2.0101\nEpoch [47/30], Loss: 2.0084\nEpoch [48/30], Loss: 2.0107\nEpoch [49/30], Loss: 2.0154\nEpoch [50/30], Loss: 2.0095\nEpoch [51/30], Loss: 2.0068\nEpoch [52/30], Loss: 2.0114\nEpoch [53/30], Loss: 2.0094\nEpoch [54/30], Loss: 2.0090\nEpoch [55/30], Loss: 2.0104\nEpoch [56/30], Loss: 2.0130\nEpoch [57/30], Loss: 2.0115\nEpoch [58/30], Loss: 2.0105\nEpoch [59/30], Loss: 2.0109\nEpoch [60/30], Loss: 2.0134\nEpoch [61/30], Loss: 2.0154\nEpoch [62/30], Loss: 2.0093\nEpoch [63/30], Loss: 2.0119\nEpoch [64/30], Loss: 2.0159\nEpoch [65/30], Loss: 2.0050\nEpoch [66/30], Loss: 2.0080\nEpoch [67/30], Loss: 2.0118\nEpoch [68/30], Loss: 2.0080\nEpoch [69/30], Loss: 2.0124\nEpoch [70/30], Loss: 2.0118\nEpoch [71/30], Loss: 2.0138\nEpoch [72/30], Loss: 2.0097\nEpoch [73/30], Loss: 2.0112\nEpoch [74/30], Loss: 2.0108\nEpoch [75/30], Loss: 2.0119\nEpoch [76/30], Loss: 2.0139\nEpoch [77/30], Loss: 2.0073\nEpoch [78/30], Loss: 2.0137\nEpoch [79/30], Loss: 2.0105\nEpoch [80/30], Loss: 2.0077\nEpoch [81/30], Loss: 2.0106\nEpoch [82/30], Loss: 2.0156\nEpoch [83/30], Loss: 2.0145\nEpoch [84/30], Loss: 2.0069\nEpoch [85/30], Loss: 2.0096\nEpoch [86/30], Loss: 2.0111\nEpoch [87/30], Loss: 2.0141\nEpoch [88/30], Loss: 2.0111\nEpoch [89/30], Loss: 2.0106\nEpoch [90/30], Loss: 2.0080\nEpoch [91/30], Loss: 2.0107\nEpoch [92/30], Loss: 2.0093\nEpoch [93/30], Loss: 2.0084\nEpoch [94/30], Loss: 2.0098\nEpoch [95/30], Loss: 2.0088\nEpoch [96/30], Loss: 2.0099\nEpoch [97/30], Loss: 2.0104\nEpoch [98/30], Loss: 2.0100\nEpoch [99/30], Loss: 2.0078\nEpoch [100/30], Loss: 2.0100\nEpoch [101/30], Loss: 2.0082\nEpoch [102/30], Loss: 2.0146\nEpoch [103/30], Loss: 2.0085\nEpoch [104/30], Loss: 2.0167\nEpoch [105/30], Loss: 2.0125\nEpoch [106/30], Loss: 2.0130\nEpoch [107/30], Loss: 2.0089\nEpoch [108/30], Loss: 2.0131\nEpoch [109/30], Loss: 2.0050\nEpoch [110/30], Loss: 2.0098\nEpoch [111/30], Loss: 2.0121\nEpoch [112/30], Loss: 2.0069\nEpoch [113/30], Loss: 2.0054\nEpoch [114/30], Loss: 2.0087\nEpoch [115/30], Loss: 2.0100\nEpoch [116/30], Loss: 2.0064\nEpoch [117/30], Loss: 2.0086\nEpoch [118/30], Loss: 2.0078\nEpoch [119/30], Loss: 2.0111\nEpoch [120/30], Loss: 2.0140\nEpoch [121/30], Loss: 2.0091\nEpoch [122/30], Loss: 2.0100\nEpoch [123/30], Loss: 2.0109\nEpoch [124/30], Loss: 2.0115\nEpoch [125/30], Loss: 2.0150\nEpoch [126/30], Loss: 2.0151\nEpoch [127/30], Loss: 2.0078\nEpoch [128/30], Loss: 2.0124\nEpoch [129/30], Loss: 2.0055\nEpoch [130/30], Loss: 2.0159\nEpoch [131/30], Loss: 2.0137\nEpoch [132/30], Loss: 2.0117\nEpoch [133/30], Loss: 2.0112\nEpoch [134/30], Loss: 2.0135\nEpoch [135/30], Loss: 2.0072\nEpoch [136/30], Loss: 2.0068\nEpoch [137/30], Loss: 2.0119\nEpoch [138/30], Loss: 2.0059\nEpoch [139/30], Loss: 2.0081\nEpoch [140/30], Loss: 2.0111\nEpoch [141/30], Loss: 2.0129\nEpoch [142/30], Loss: 2.0060\nEpoch [143/30], Loss: 2.0080\nEpoch [144/30], Loss: 2.0065\nEpoch [145/30], Loss: 2.0044\nEpoch [146/30], Loss: 2.0153\nEpoch [147/30], Loss: 2.0113\nEpoch [148/30], Loss: 2.0126\nEpoch [149/30], Loss: 2.0108\nEpoch [150/30], Loss: 2.0094\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluation loop\nmodel1.eval()  # Set the model to evaluation mode\ncorrect = 0\ntotal = 0\n\n# Use no_grad() to disable gradient calculation\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n      # Ensure correct shape [batch_size, channels, height, width]\n        inputs = inputs.unsqueeze(1) \n        # Forward pass through the model\n        outputs = model1(inputs)  # Get model predictions\n        \n        # Get the predicted class (the index of the maximum log-probability)\n        _, predicted = torch.max(outputs.data, 1)\n        \n        # Update total and correct counts\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()  # Count correct predictions\n\n# Print the final accuracy\naccuracy = 100 * correct / total if total > 0 else 0\nprint(f'Accuracy: {accuracy:.2f}%')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T23:32:43.432407Z","iopub.execute_input":"2024-10-18T23:32:43.433250Z","iopub.status.idle":"2024-10-18T23:32:43.533310Z","shell.execute_reply.started":"2024-10-18T23:32:43.433204Z","shell.execute_reply":"2024-10-18T23:32:43.532332Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 16.09%\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"X_tensor.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T23:33:19.016446Z","iopub.execute_input":"2024-10-18T23:33:19.016866Z","iopub.status.idle":"2024-10-18T23:33:19.024146Z","shell.execute_reply.started":"2024-10-18T23:33:19.016827Z","shell.execute_reply":"2024-10-18T23:33:19.022843Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"torch.Size([12798, 40, 100, 1])"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"dataset = TensorDataset(X_tensor, y_tensor)\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T23:37:27.751110Z","iopub.execute_input":"2024-10-18T23:37:27.751556Z","iopub.status.idle":"2024-10-18T23:37:27.758737Z","shell.execute_reply.started":"2024-10-18T23:37:27.751520Z","shell.execute_reply":"2024-10-18T23:37:27.757900Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"class ComplexModel(nn.Module):\n    def __init__(self):\n        super(ComplexModel, self).__init__()\n        \n        self.conv1 = nn.Conv1d(in_channels=100, out_channels=2048, kernel_size=5, padding='same')\n        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n        self.bn1 = nn.BatchNorm1d(2048)\n\n        self.conv2 = nn.Conv1d(in_channels=2048, out_channels=1024, kernel_size=5, padding='same')\n        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n        self.bn2 = nn.BatchNorm1d(1024)\n\n        self.conv3 = nn.Conv1d(in_channels=1024, out_channels=512, kernel_size=5, padding='same')\n        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n        self.bn3 = nn.BatchNorm1d(512)\n\n        self.lstm1 = nn.LSTM(input_size=512, hidden_size=256, num_layers=1, batch_first=True, bidirectional=False, dropout=0.3)\n        self.lstm2 = nn.LSTM(input_size=256, hidden_size=128, num_layers=1, batch_first=True, bidirectional=False)\n\n        self.fc1 = nn.Linear(128, 128)\n        self.dropout1 = nn.Dropout(0.5)\n\n        self.fc2 = nn.Linear(128, 64)\n        self.dropout2 = nn.Dropout(0.5)\n\n        self.fc3 = nn.Linear(64, 32)\n        self.dropout3 = nn.Dropout(0.2)\n\n        self.fc4 = nn.Linear(32, 16)  # Adjust the output dimension as needed\n\n    def forward(self, x):\n        # x shape: (batch_size, 40, 100, 1) -> reshape to (batch_size, 100, 40)\n        x = x.squeeze(-1)  # Shape: (batch_size, 100, 40)\n        x = x.permute(0, 2, 1)  # Shape: (batch_size, 40, 100)\n        \n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.pool1(x)\n        \n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.pool2(x)\n        \n        x = F.relu(self.bn3(self.conv3(x)))\n        x = self.pool3(x)\n\n        # x shape after Conv1D and MaxPool: (batch_size, 512, new_length)\n        # Prepare for LSTM: need (batch_size, seq_length, features)\n        x = x.permute(0, 2, 1)  # Shape: (batch_size, new_length, 512)\n\n        lstm_out, _ = self.lstm1(x)  # LSTM output\n        lstm_out, _ = self.lstm2(lstm_out)  # LSTM output\n\n        # Take the last output from LSTM\n        x = lstm_out[:, -1, :]  # Shape: (batch_size, 128)\n\n        x = self.fc1(x)\n        x = self.dropout1(x)\n        x = F.relu(x)\n\n        x = self.fc2(x)\n        x = self.dropout2(x)\n        x = F.relu(x)\n\n        x = self.fc3(x)\n        x = self.dropout3(x)\n        x = F.relu(x)\n\n        x = self.fc4(x)  # Output layer\n        return x\n\n# Initialize model\nmodel = ComplexModel().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T23:39:12.424984Z","iopub.execute_input":"2024-10-18T23:39:12.425697Z","iopub.status.idle":"2024-10-18T23:39:12.597431Z","shell.execute_reply.started":"2024-10-18T23:39:12.425650Z","shell.execute_reply":"2024-10-18T23:39:12.596499Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"import torch.nn.functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T23:38:18.970727Z","iopub.execute_input":"2024-10-18T23:38:18.971676Z","iopub.status.idle":"2024-10-18T23:38:18.976096Z","shell.execute_reply.started":"2024-10-18T23:38:18.971632Z","shell.execute_reply":"2024-10-18T23:38:18.974936Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"for epoch in range(100):\n        total_loss = 0\n        total_correct = 0\n        total_samples = 0\n\n        for inputs, labels in train_loader:\n            optimizer.zero_grad()  # Zero the gradients\n            \n            outputs = model(inputs)  # Forward pass\n            loss = criterion(outputs, labels)  # Compute loss\n            loss.backward()  # Backward pass\n            optimizer.step()  # Update weights\n            \n            total_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)  # Get predictions\n            total_correct += (predicted == labels).sum().item()\n            total_samples += labels.size(0)\n\n        # Calculate average loss and accuracy\n        avg_loss = total_loss / len(train_loader)\n        accuracy = total_correct / total_samples * 100\n\n        print(f'Epoch [{epoch + 1}/{100}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T23:39:36.652451Z","iopub.execute_input":"2024-10-18T23:39:36.652858Z","iopub.status.idle":"2024-10-18T23:50:16.277612Z","shell.execute_reply.started":"2024-10-18T23:39:36.652820Z","shell.execute_reply":"2024-10-18T23:50:16.275909Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/100], Loss: 2.7436, Accuracy: 15.15%\nEpoch [2/100], Loss: 2.7429, Accuracy: 15.00%\nEpoch [3/100], Loss: 2.7430, Accuracy: 15.48%\nEpoch [4/100], Loss: 2.7434, Accuracy: 14.95%\nEpoch [5/100], Loss: 2.7433, Accuracy: 14.89%\nEpoch [6/100], Loss: 2.7431, Accuracy: 15.28%\nEpoch [7/100], Loss: 2.7429, Accuracy: 15.64%\nEpoch [8/100], Loss: 2.7429, Accuracy: 15.05%\nEpoch [9/100], Loss: 2.7429, Accuracy: 15.46%\nEpoch [10/100], Loss: 2.7433, Accuracy: 14.92%\nEpoch [11/100], Loss: 2.7430, Accuracy: 15.48%\nEpoch [12/100], Loss: 2.7429, Accuracy: 15.65%\nEpoch [13/100], Loss: 2.7433, Accuracy: 15.19%\nEpoch [14/100], Loss: 2.7432, Accuracy: 15.10%\nEpoch [15/100], Loss: 2.7429, Accuracy: 15.14%\nEpoch [16/100], Loss: 2.7433, Accuracy: 15.53%\nEpoch [17/100], Loss: 2.7432, Accuracy: 15.59%\nEpoch [18/100], Loss: 2.7431, Accuracy: 15.31%\nEpoch [19/100], Loss: 2.7430, Accuracy: 15.47%\nEpoch [20/100], Loss: 2.7429, Accuracy: 15.34%\nEpoch [21/100], Loss: 2.7433, Accuracy: 14.85%\nEpoch [22/100], Loss: 2.7432, Accuracy: 15.13%\nEpoch [23/100], Loss: 2.7432, Accuracy: 14.85%\nEpoch [24/100], Loss: 2.7436, Accuracy: 15.07%\nEpoch [25/100], Loss: 2.7431, Accuracy: 14.92%\nEpoch [26/100], Loss: 2.7429, Accuracy: 15.35%\nEpoch [27/100], Loss: 2.7433, Accuracy: 14.77%\nEpoch [28/100], Loss: 2.7427, Accuracy: 15.69%\nEpoch [29/100], Loss: 2.7434, Accuracy: 14.81%\nEpoch [30/100], Loss: 2.7431, Accuracy: 15.43%\nEpoch [31/100], Loss: 2.7430, Accuracy: 15.21%\nEpoch [32/100], Loss: 2.7431, Accuracy: 15.15%\nEpoch [33/100], Loss: 2.7428, Accuracy: 15.20%\nEpoch [34/100], Loss: 2.7432, Accuracy: 15.14%\nEpoch [35/100], Loss: 2.7434, Accuracy: 15.36%\nEpoch [36/100], Loss: 2.7433, Accuracy: 15.11%\nEpoch [37/100], Loss: 2.7432, Accuracy: 15.14%\nEpoch [38/100], Loss: 2.7430, Accuracy: 15.86%\nEpoch [39/100], Loss: 2.7434, Accuracy: 15.03%\nEpoch [40/100], Loss: 2.7432, Accuracy: 15.05%\nEpoch [41/100], Loss: 2.7431, Accuracy: 15.24%\nEpoch [42/100], Loss: 2.7430, Accuracy: 15.30%\nEpoch [43/100], Loss: 2.7432, Accuracy: 15.61%\nEpoch [44/100], Loss: 2.7429, Accuracy: 15.47%\nEpoch [45/100], Loss: 2.7431, Accuracy: 15.24%\nEpoch [46/100], Loss: 2.7429, Accuracy: 15.12%\nEpoch [47/100], Loss: 2.7431, Accuracy: 15.48%\nEpoch [48/100], Loss: 2.7434, Accuracy: 15.29%\nEpoch [49/100], Loss: 2.7433, Accuracy: 15.10%\nEpoch [50/100], Loss: 2.7432, Accuracy: 15.40%\nEpoch [51/100], Loss: 2.7434, Accuracy: 15.64%\nEpoch [52/100], Loss: 2.7432, Accuracy: 14.88%\nEpoch [53/100], Loss: 2.7432, Accuracy: 15.11%\nEpoch [54/100], Loss: 2.7431, Accuracy: 15.00%\nEpoch [55/100], Loss: 2.7430, Accuracy: 15.56%\nEpoch [56/100], Loss: 2.7431, Accuracy: 14.97%\nEpoch [57/100], Loss: 2.7431, Accuracy: 15.35%\nEpoch [58/100], Loss: 2.7433, Accuracy: 15.19%\nEpoch [59/100], Loss: 2.7431, Accuracy: 15.57%\nEpoch [60/100], Loss: 2.7432, Accuracy: 15.57%\nEpoch [61/100], Loss: 2.7433, Accuracy: 15.31%\nEpoch [62/100], Loss: 2.7431, Accuracy: 15.26%\nEpoch [63/100], Loss: 2.7434, Accuracy: 15.21%\nEpoch [64/100], Loss: 2.7431, Accuracy: 15.74%\nEpoch [65/100], Loss: 2.7430, Accuracy: 15.57%\nEpoch [66/100], Loss: 2.7429, Accuracy: 15.39%\nEpoch [67/100], Loss: 2.7432, Accuracy: 15.31%\nEpoch [68/100], Loss: 2.7430, Accuracy: 15.66%\nEpoch [69/100], Loss: 2.7433, Accuracy: 14.86%\nEpoch [70/100], Loss: 2.7429, Accuracy: 15.09%\nEpoch [71/100], Loss: 2.7430, Accuracy: 15.41%\nEpoch [72/100], Loss: 2.7432, Accuracy: 15.17%\nEpoch [73/100], Loss: 2.7433, Accuracy: 15.48%\nEpoch [74/100], Loss: 2.7431, Accuracy: 14.99%\nEpoch [75/100], Loss: 2.7432, Accuracy: 15.22%\nEpoch [76/100], Loss: 2.7431, Accuracy: 15.48%\nEpoch [77/100], Loss: 2.7432, Accuracy: 15.54%\nEpoch [78/100], Loss: 2.7431, Accuracy: 15.38%\nEpoch [79/100], Loss: 2.7433, Accuracy: 15.23%\nEpoch [80/100], Loss: 2.7431, Accuracy: 15.56%\nEpoch [81/100], Loss: 2.7432, Accuracy: 15.38%\nEpoch [82/100], Loss: 2.7431, Accuracy: 15.54%\nEpoch [83/100], Loss: 2.7435, Accuracy: 15.12%\nEpoch [84/100], Loss: 2.7433, Accuracy: 15.13%\nEpoch [85/100], Loss: 2.7433, Accuracy: 15.06%\nEpoch [86/100], Loss: 2.7431, Accuracy: 15.69%\nEpoch [87/100], Loss: 2.7431, Accuracy: 15.60%\nEpoch [88/100], Loss: 2.7431, Accuracy: 15.15%\nEpoch [89/100], Loss: 2.7432, Accuracy: 15.54%\nEpoch [90/100], Loss: 2.7433, Accuracy: 15.41%\nEpoch [91/100], Loss: 2.7431, Accuracy: 15.19%\nEpoch [92/100], Loss: 2.7435, Accuracy: 15.09%\nEpoch [93/100], Loss: 2.7431, Accuracy: 15.51%\nEpoch [94/100], Loss: 2.7431, Accuracy: 14.96%\nEpoch [95/100], Loss: 2.7432, Accuracy: 14.78%\nEpoch [96/100], Loss: 2.7432, Accuracy: 15.41%\nEpoch [97/100], Loss: 2.7430, Accuracy: 15.37%\nEpoch [98/100], Loss: 2.7430, Accuracy: 15.81%\nEpoch [99/100], Loss: 2.7431, Accuracy: 14.93%\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[45], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero the gradients\u001b[39;00m\n\u001b[1;32m      9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update weights\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m,\n\u001b[1;32m   1189\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[1;32m   1190\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1716\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1716\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1718\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":45},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}